\documentclass{article} \usepackage[margin=1in]{geometry} \usepackage{hyperref} \usepackage{graphicx} % --- Document Metadata --- \title{NASA Space Apps: Exoplanet Prediction AI} \author{Your Team Name} \date{\today} \begin{document} \maketitle \section*{Project Overview} This project is a machine learning web application built for the NASA Space Apps Challenge. It uses data from NASA's Kepler mission to predict whether a stellar signal corresponds to a potential exoplanet. The application consists of two main parts: \begin{enumerate} \item A Python script that trains a Random Forest classifier on the Kepler exoplanet dataset. \item An interactive web application built with Streamlit that allows users to input signal characteristics and get a live prediction from the trained model. \end{enumerate} \begin{figure}[h!] \centering % Placeholder for an image of your app % \includegraphics[width=0.8\textwidth]{app_screenshot.png} \fbox{Image of the Exoplanet Predictor App} \caption{A screenshot of the running Streamlit application.} \end{figure} \subsection*{Features} \begin{itemize} \item \textbf{High-Accuracy Model:} The Random Forest model is trained on NASA's Kepler data and achieves \~93\% accuracy on the test set. \item \textbf{Interactive Web Interface:} A user-friendly web app allows for real-time predictions by adjusting sliders and input fields. \item \textbf{Dark Mode Theme:} A sleek, modern "bare mode" interface for better usability. \item \textbf{Self-Contained:} The project includes scripts to both train the model from scratch and run the user-facing application. \end{itemize} \subsection*{Technology Stack} \begin{itemize} \item \textbf{Backend \& Model:} Python \item \textbf{Machine Learning:} Scikit-learn \item \textbf{Data Manipulation:} Pandas, NumPy \item \textbf{Web Framework:} Streamlit \item \textbf{Model Serialization:} Joblib \end{itemize} \subsection*{Project Structure} \begin{verbatim} . ‚îú‚îÄ‚îÄ üìÑ cumulative.csv # The NASA Kepler dataset for training. ‚îú‚îÄ‚îÄ üêç train_model.py # Script to train the ML model and save it. ‚îú‚îÄ‚îÄ üñ•Ô∏è app.py # The Streamlit web application script. ‚îú‚îÄ‚îÄ üß† planet_model.joblib # The saved, pre-trained ML model. ‚îú‚îÄ‚îÄ ‚öñÔ∏è scaler.joblib # The saved scaler for data normalization. ‚îî‚îÄ‚îÄ üìñ README.md # Project documentation. \end{verbatim} \section*{Setup and Installation} Follow these steps to get the project running on your local machine. \subsection*{1. Prerequisites} \begin{itemize} \item Python 3.8 or newer installed. \item \texttt{pip} (Python package installer). \end{itemize} \subsection*{2. Clone the Repository (Optional)} If you are using Git, you can clone the repository. Otherwise, ensure all files are in the same folder. \begin{verbatim} git clone cd \end{verbatim} \subsection*{3. Install Dependencies} Install all the required Python libraries using pip: \begin{verbatim} pip install pandas scikit-learn streamlit joblib \end{verbatim} \subsection*{4. Download the Dataset} \begin{enumerate} \item Download the Kepler exoplanet dataset from \href{https://www.kaggle.com/datasets/nasa/kepler-exoplanet-search-results}{Kaggle}. \item From the download, find the \texttt{cumulative.csv} file. \item Place \texttt{cumulative.csv} in the root directory of the project. \end{enumerate} \section*{How to Run the Application} The application is run in two stages. \subsection*{Step 1: Train the Model} First, run the training script to process the dataset and create the model files (\texttt{planet\_model.joblib} and \texttt{scaler.joblib}). Open your terminal in the project directory and run: \begin{verbatim} python train_model.py \end{verbatim} You should see output confirming the model was trained successfully, with an accuracy report. \subsection*{Step 2: Launch the Web App} Once the model is trained, launch the interactive web application. In the same terminal, run: \begin{verbatim} python -m streamlit run app.py \end{verbatim} Your default web browser will automatically open a new tab with the application running. \end{document}
